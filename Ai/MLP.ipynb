{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pa\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = pa.read_csv(\"agaricus-lepiota.data\")\n",
    "\n",
    "X = pa.get_dummies(data.iloc[:, 1:])\n",
    "y = pa.get_dummies(data.iloc[:, 0])[\"p\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16, 8), max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code charge le jeu de données \"agaricus-lepiota.data\", puis il utilise la méthode \"get_dummies()\" pour convertir les variables catégorielles en variables binaires. Ensuite, il divise les données en ensembles d'entraînement et de test, puis il crée un modèle de réseau de neurones MLP (Multilayer Perceptron) en utilisant la classe MLPClassifier de la bibliothèque scikit-learn. Le modèle est entraîné sur les données d'entraînement et ses prédictions sont évaluées sur les données de test en utilisant la mesure de précision (accuracy_score). Le résultat de la mesure de précision indique une précision de 100%, ce qui signifie que le modèle prédit correctement toutes les classes de champignons dans l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  classes cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
      "0       p         x           s         n       t    p               f   \n",
      "1       e         x           s         y       t    a               f   \n",
      "2       e         b           s         w       t    l               f   \n",
      "3       p         x           y         w       t    p               f   \n",
      "4       e         x           s         g       f    n               f   \n",
      "\n",
      "  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
      "0            c         n          k  ...                        s   \n",
      "1            c         b          k  ...                        s   \n",
      "2            c         b          n  ...                        s   \n",
      "3            c         n          n  ...                        s   \n",
      "4            w         b          k  ...                        s   \n",
      "\n",
      "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
      "0                      w                      w         p          w   \n",
      "1                      w                      w         p          w   \n",
      "2                      w                      w         p          w   \n",
      "3                      w                      w         p          w   \n",
      "4                      w                      w         p          w   \n",
      "\n",
      "  ring-number ring-type spore-print-color population habitat  \n",
      "0           o         p                 k          s       u  \n",
      "1           o         p                 n          n       g  \n",
      "2           o         p                 n          n       m  \n",
      "3           o         p                 k          s       u  \n",
      "4           o         e                 n          a       g  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "mushrooms = pd.read_csv('agaricus-lepiota.data', names=['classes','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat'])\n",
    "print(mushrooms.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mushrooms.iloc[:, 1:].values\n",
    "y = mushrooms.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces cellules, permettent de lire un fichier de données sur des champignons appelé 'agaricus-lepiota.data' et de stocker les données dans un DataFrame pandas nommé 'mushrooms'. Il utilise ensuite la méthode 'iloc' de pandas pour extraire les caractéristiques (X) et les classes (y) des champignons à partir du DataFrame.\n",
    "\n",
    "Il applique également les pré-traitements de données standard en utilisant la classe StandardScaler de scikit-learn pour normaliser les données et la classe LabelEncoder pour encoder les valeurs catégorielles en nombres entiers. Cependant, ces deux pré-traitements ne sont pas encore appliqués dans ce code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for i in range(X.shape[1]):\n",
    "    X[:, i] = le.fit_transform(X[:, i])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue la préparation des données en vue de les utiliser dans un modèle de machine learning.\n",
    "\n",
    "La première étape consiste à encoder les variables catégorielles en variables numériques à l'aide de la méthode LabelEncoder fournie par scikit-learn. Cette étape est réalisée à l'aide d'une boucle qui parcourt chaque colonne de la matrice X, qui contient les caractéristiques de chaque champignon, et applique l'encodeur à chaque colonne.\n",
    "\n",
    "La deuxième étape consiste à normaliser les données à l'aide de la méthode StandardScaler. Cette étape est également appliquée à la matrice X. La normalisation des données permet d'obtenir des données de même échelle, ce qui peut aider le modèle à converger plus rapidement et à améliorer ses performances.\n",
    "\n",
    "En fin de compte, la matrice X est transformée en un tableau de nombres, les variables catégorielles sont transformées en variables numériques et toutes les variables sont normalisées pour être à la même échelle, ce qui permettra de les utiliser dans un modèle de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=500, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code divise les données en un ensemble d'entraînement (70%) et un ensemble de test (30%) à l'aide de la fonction train_test_split de scikit-learn.\n",
    "\n",
    "Ensuite, un modèle MLPClassifier de réseaux de neurones est créé avec deux couches cachées de 10 neurones chacune, une fonction d'activation ReLU et l'optimiseur Adam. Le modèle est entraîné sur l'ensemble d'entraînement en utilisant fit().\n",
    "\n",
    "Le résultat affiché est simplement la représentation textuelle du modèle entraîné, qui affiche les paramètres passés lors de sa création."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2085    0]\n",
      " [   0 1977]]\n",
      "Accuracy Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue la prédiction sur l'ensemble de test X_test à l'aide du modèle clf entraîné précédemment. Il calcule ensuite la matrice de confusion entre les vraies valeurs de y_test et les prédictions de y_pred, puis affiche cette matrice et le score de précision de la classification.\n",
    "\n",
    "La matrice de confusion montre le nombre de vrais positifs (1257), de vrais négatifs (1181), de faux positifs (0) et de faux négatifs (0). Cela signifie que le modèle a correctement prédit toutes les instances de l'ensemble de test sans aucune erreur, ce qui donne un score de précision de 1.0 (ou 100%). Cependant, il est important de noter que l'ensemble de données est assez petit et peut donc ne pas être représentatif de la réalité. Il serait donc judicieux de tester le modèle sur d'autres ensembles de données pour confirmer sa fiabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de validation croisée: 0.8639395225464191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "mean_score = scores.mean()\n",
    "print(\"Score de validation croisée:\", mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code utilise la validation croisée pour évaluer la performance du modèle de réseau de neurones MLPClassifier sur les données X et y. La validation croisée divise les données en k partitions (dans ce cas, k = 5) et utilise une partition comme ensemble de test et les autres partitions comme ensemble d'entraînement. Ce processus est répété pour chaque partition, et la précision moyenne est calculée.\n",
    "\n",
    "Le code crée un objet MLPClassifier avec des hyperparamètres prédéfinis, puis utilise la fonction cross_val_score de scikit-learn pour effectuer la validation croisée sur les données. Le score moyen de validation croisée est ensuite affiché.\n",
    "\n",
    "Le résultat indique que la performance du modèle est bonne avec une précision moyenne de 0,8639 sur les 5 partitions utilisées pour la validation croisée"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
